{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, ReLU\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/miniimagenet/'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'train')\n",
    "valid_dir = os.path.join(data_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "IMG_CHANNELS = 3\n",
    "N_CLASSES = len(os.listdir(train_dir))\n",
    "N_INNER_CLASSES = 5\n",
    "INNER_EPOCHS = 1\n",
    "INNER_BATCH_SIZE = 10\n",
    "N_INNER_STEPS = 8\n",
    "INNER_LR = 0.001\n",
    "OUTER_EPOCHS = 100000\n",
    "OUTER_LR = 0.1\n",
    "OUTER_BATCH_SIZE = 5\n",
    "DISPLAY_FREQ = 50\n",
    "N_VALID_TESTS = 50\n",
    "MEM_LIMIT = 12 * 1000 * 1000 * 1000\n",
    "IMG_BYTES = 98304\n",
    "LOAD_MODEL = True\n",
    "MODEL_NAME = 'Model-V3.hdf5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Image Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img, size=IMG_SIZE):\n",
    "    return cv2.resize(img, (size, size))\n",
    "\n",
    "def load_img(class_dir=None, img_name=None, img_path=None):\n",
    "    if img_path:\n",
    "        img_name = img_path[img_path.rfind('/')+1:]\n",
    "    else:\n",
    "        img_path = os.path.join(class_dir, img_name[:img_name.find('_')], img_name)\n",
    "    \n",
    "    # If image is already loaded into cache return it\n",
    "    if img_name in img_cache:\n",
    "        return img_cache[img_name]\n",
    "    else:\n",
    "        img = cv2.imread(img_path)\n",
    "        img = preprocess_img(img, size=IMG_SIZE)\n",
    "        img = img / 255.\n",
    "        if len(img_cache) * IMG_BYTES < MEM_LIMIT:\n",
    "            img_cache[img_name] = img\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the DataFrame of Image Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = np.asarray(os.listdir(train_dir))\n",
    "\n",
    "cols = ['class', 'img_name']\n",
    "df_train = pd.DataFrame(columns=cols)\n",
    "\n",
    "for cat in train_classes:\n",
    "    class_list = []\n",
    "    cat_dir = os.listdir(os.path.join(train_dir, cat))\n",
    "    for img_name in cat_dir:\n",
    "        class_list.append([cat, img_name])\n",
    "    tmp_df = pd.DataFrame(class_list, columns=cols)\n",
    "    df_train = df_train.append(tmp_df)\n",
    "    \n",
    "df_train = df_train.sample(frac=1)\n",
    "df_train.reset_index(inplace=True)\n",
    "df_train.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same for Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_classes = np.asarray(os.listdir(train_dir))\n",
    "\n",
    "cols = ['class', 'img_name']\n",
    "df_valid = pd.DataFrame(columns=cols)\n",
    "\n",
    "for cat in train_classes:\n",
    "    class_list = []\n",
    "    cat_dir = os.listdir(os.path.join(train_dir, cat))\n",
    "    for img_name in cat_dir:\n",
    "        class_list.append([cat, img_name])\n",
    "    tmp_df = pd.DataFrame(class_list, columns=cols)\n",
    "    df_valid = df_valid.append(tmp_df)\n",
    "    \n",
    "df_valid = df_train.sample(frac=1)\n",
    "df_valid.reset_index(inplace=True)\n",
    "df_valid.drop('index', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Generating Tasks and Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_task(data, n_classes=N_INNER_CLASSES):\n",
    "    classes = np.random.choice(train_classes, size=n_classes, replace=False)\n",
    "    task_indices = data['class'].map(lambda x: x in classes)\n",
    "    task_data = data[task_indices]\n",
    "    return task_data\n",
    "\n",
    "def gen_batches(task_data, batch_size=INNER_BATCH_SIZE, data_dir=train_dir):\n",
    "    task_data = pd.concat([task_data, pd.get_dummies(task_data['class'])], axis=1)\n",
    "    while True:\n",
    "        epoch_data = task_data.sample(frac=1)\n",
    "        for i in range((len(epoch_data) // batch_size) - 1):\n",
    "            X_names = epoch_data.iloc[i*batch_size:(i+1)*batch_size]['img_name'].values\n",
    "            X = np.asarray([load_img(data_dir, img_name) for img_name in X_names])\n",
    "            y = epoch_data.drop(['class', 'img_name'], axis=1).iloc[i*batch_size:(i+1)*batch_size].values\n",
    "            \n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "re_lu_10 (ReLU)              (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "re_lu_11 (ReLU)              (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "re_lu_12 (ReLU)              (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 10245     \n",
      "=================================================================\n",
      "Total params: 30,021\n",
      "Trainable params: 29,829\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(IMG_SIZE, IMG_SIZE, IMG_CHANNELS)))\n",
    "\n",
    "for i in range(3):\n",
    "    model.add(Conv2D(32, 3, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D((2, 2), padding='same'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(N_INNER_CLASSES, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=INNER_LR),\n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    model.load_weights('models/' + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [ReduceLROnPlateau(monitor='val_loss', factor=0.1, \n",
    "                               patience=15, verbose=0, \n",
    "                               mode='auto', min_delta=0.0001, \n",
    "                               cooldown=10, min_lr=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50\n",
      "loss: 1.3597\n",
      "acc: 0.3935\n",
      "val_loss: 1.3651\n",
      "val_acc: 0.3960\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 100\n",
      "loss: 1.3682\n",
      "acc: 0.3970\n",
      "val_loss: 1.3523\n",
      "val_acc: 0.3940\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 150\n",
      "loss: 1.3450\n",
      "acc: 0.4021\n",
      "val_loss: 1.3642\n",
      "val_acc: 0.3978\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 200\n",
      "loss: 1.3477\n",
      "acc: 0.4068\n",
      "val_loss: 1.3840\n",
      "val_acc: 0.3715\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 250\n",
      "loss: 1.3519\n",
      "acc: 0.4025\n",
      "val_loss: 1.3630\n",
      "val_acc: 0.4055\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 300\n",
      "loss: 1.3367\n",
      "acc: 0.4089\n",
      "val_loss: 1.3459\n",
      "val_acc: 0.4070\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 350\n",
      "loss: 1.3559\n",
      "acc: 0.4024\n",
      "val_loss: 1.3453\n",
      "val_acc: 0.4090\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 400\n",
      "loss: 1.3467\n",
      "acc: 0.4066\n",
      "val_loss: 1.3303\n",
      "val_acc: 0.4263\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 450\n",
      "loss: 1.3566\n",
      "acc: 0.4004\n",
      "val_loss: 1.3372\n",
      "val_acc: 0.4135\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 500\n",
      "loss: 1.3440\n",
      "acc: 0.4023\n",
      "val_loss: 1.3595\n",
      "val_acc: 0.3925\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 550\n",
      "loss: 1.3358\n",
      "acc: 0.4085\n",
      "val_loss: 1.3393\n",
      "val_acc: 0.4128\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 600\n",
      "loss: 1.3501\n",
      "acc: 0.4058\n",
      "val_loss: 1.3922\n",
      "val_acc: 0.3880\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 650\n",
      "loss: 1.3401\n",
      "acc: 0.4118\n",
      "val_loss: 1.3468\n",
      "val_acc: 0.4115\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 700\n",
      "loss: 1.3449\n",
      "acc: 0.4071\n",
      "val_loss: 1.3311\n",
      "val_acc: 0.4133\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 750\n",
      "loss: 1.3403\n",
      "acc: 0.4086\n",
      "val_loss: 1.3380\n",
      "val_acc: 0.4143\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 800\n",
      "loss: 1.3410\n",
      "acc: 0.4096\n",
      "val_loss: 1.3669\n",
      "val_acc: 0.3900\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 850\n",
      "loss: 1.3570\n",
      "acc: 0.4013\n",
      "val_loss: 1.3406\n",
      "val_acc: 0.4143\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 900\n",
      "loss: 1.3298\n",
      "acc: 0.4131\n",
      "val_loss: 1.3280\n",
      "val_acc: 0.4228\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 950\n",
      "loss: 1.3128\n",
      "acc: 0.4259\n",
      "val_loss: 1.3334\n",
      "val_acc: 0.4115\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1000\n",
      "loss: 1.3462\n",
      "acc: 0.4031\n",
      "val_loss: 1.3510\n",
      "val_acc: 0.3948\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 1050\n",
      "loss: 1.3137\n",
      "acc: 0.4243\n",
      "val_loss: 1.3446\n",
      "val_acc: 0.3938\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1100\n",
      "loss: 1.3438\n",
      "acc: 0.4050\n",
      "val_loss: 1.3188\n",
      "val_acc: 0.4200\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1150\n",
      "loss: 1.3316\n",
      "acc: 0.4130\n",
      "val_loss: 1.3463\n",
      "val_acc: 0.4140\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1200\n",
      "loss: 1.3249\n",
      "acc: 0.4163\n",
      "val_loss: 1.3357\n",
      "val_acc: 0.4150\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 1250\n",
      "loss: 1.3342\n",
      "acc: 0.4155\n",
      "val_loss: 1.3743\n",
      "val_acc: 0.3883\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1300\n",
      "loss: 1.3240\n",
      "acc: 0.4162\n",
      "val_loss: 1.3449\n",
      "val_acc: 0.4028\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1350\n",
      "loss: 1.3217\n",
      "acc: 0.4169\n",
      "val_loss: 1.3290\n",
      "val_acc: 0.4138\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1400\n",
      "loss: 1.3289\n",
      "acc: 0.4156\n",
      "val_loss: 1.3476\n",
      "val_acc: 0.4003\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 1450\n",
      "loss: 1.3371\n",
      "acc: 0.4069\n",
      "val_loss: 1.3536\n",
      "val_acc: 0.4035\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1500\n",
      "loss: 1.3200\n",
      "acc: 0.4227\n",
      "val_loss: 1.3388\n",
      "val_acc: 0.4068\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1550\n",
      "loss: 1.3467\n",
      "acc: 0.4065\n",
      "val_loss: 1.3405\n",
      "val_acc: 0.4080\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1600\n",
      "loss: 1.3102\n",
      "acc: 0.4188\n",
      "val_loss: 1.3430\n",
      "val_acc: 0.4160\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 1650\n",
      "loss: 1.3343\n",
      "acc: 0.4103\n",
      "val_loss: 1.3461\n",
      "val_acc: 0.3968\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1700\n",
      "loss: 1.3294\n",
      "acc: 0.4093\n",
      "val_loss: 1.3770\n",
      "val_acc: 0.3745\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1750\n",
      "loss: 1.3262\n",
      "acc: 0.4166\n",
      "val_loss: 1.3852\n",
      "val_acc: 0.3863\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1800\n",
      "loss: 1.3322\n",
      "acc: 0.4089\n",
      "val_loss: 1.3368\n",
      "val_acc: 0.4165\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n",
      "Epoch 1850\n",
      "loss: 1.3257\n",
      "acc: 0.4099\n",
      "val_loss: 1.3453\n",
      "val_acc: 0.4000\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1900\n",
      "loss: 1.3391\n",
      "acc: 0.4086\n",
      "val_loss: 1.3663\n",
      "val_acc: 0.3860\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 1950\n",
      "loss: 1.3180\n",
      "acc: 0.4168\n",
      "val_loss: 1.3618\n",
      "val_acc: 0.3865\n",
      "lr: 0.0001\n",
      "\n",
      "Epoch 2000\n",
      "loss: 1.3294\n",
      "acc: 0.4139\n",
      "val_loss: 1.3414\n",
      "val_acc: 0.4190\n",
      "lr: 0.0001\n",
      "\n",
      "\n",
      "Saving Model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hists = []\n",
    "\n",
    "for i in range(OUTER_EPOCHS):\n",
    "    \n",
    "    initial_weights = model.get_weights()\n",
    "    \n",
    "    for _ in range(OUTER_BATCH_SIZE):\n",
    "        task_train = sample_task(df_train)\n",
    "        data_train = gen_batches(task_train, data_dir=train_dir)\n",
    "\n",
    "        task_valid = sample_task(df_valid)\n",
    "        data_valid = gen_batches(task_valid, batch_size=INNER_BATCH_SIZE*N_INNER_STEPS//5, data_dir=valid_dir)\n",
    "        batch_valid = next(data_valid)\n",
    "        \n",
    "        hist = model.fit_generator(data_train,\n",
    "                  steps_per_epoch=N_INNER_STEPS,\n",
    "                  validation_data=batch_valid,\n",
    "                  epochs=1,\n",
    "                  callbacks=callbacks,\n",
    "                  verbose=0)\n",
    "        hists.append(hist.history)\n",
    "        \n",
    "    if i != 0 and i % DISPLAY_FREQ == 0:\n",
    "        metrics = {'loss': 0, 'acc': 0, 'val_loss': 0, 'val_acc': 0, 'lr': 0}\n",
    "        \n",
    "        for hist in hists[-OUTER_BATCH_SIZE*DISPLAY_FREQ:]:\n",
    "            for key in hist:\n",
    "                metrics[key] += hist[key][0]\n",
    "                \n",
    "        print('Epoch', i)\n",
    "        for key in metrics:\n",
    "            metrics[key] /= OUTER_BATCH_SIZE*DISPLAY_FREQ\n",
    "            print('{}: {:0.4f}'.format(key, metrics[key]))\n",
    "        print()\n",
    "        \n",
    "    if i != 0 and i % (DISPLAY_FREQ*4) == 0:\n",
    "        print('\\nSaving Model\\n')\n",
    "        model.save('models/' + MODEL_NAME)\n",
    "    \n",
    "    model.set_weights([iw + OUTER_LR * (cw - iw)\n",
    "                       for iw, cw in zip(initial_weights, model.get_weights())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/' + MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_score = 0\n",
    "for i in tqdm(range(N_VALID_TESTS)):\n",
    "    task = sample_task(df_valid)\n",
    "    data = gen_batches(task)\n",
    "    metrics = model.evaluate_generator(data, steps=N_INNER_STEPS, verbose=0)\n",
    "    valid_score += metrics[1]\n",
    "    \n",
    "print('Accuracy:', valid_score / N_VALID_TESTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.signal.savgol_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 8))\n",
    "\n",
    "plt.plot(list(range(len(hists))), [hist['acc'][0] for hist in hists], lw=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hists[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
